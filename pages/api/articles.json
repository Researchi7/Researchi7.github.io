[
  {
    "id": 1,
    "title": "Preference Alignment on Diffusion Model: A Comprehensive Survey for Image Generation and Editing",
    "conference": "IJCAI 2025 (Under Review)",
    "cover_image": "dmrl-survey.png",
    "year": "Feb, 2025",
    "abstract": "The integration of preference alignment with diffusion models (DMs) has emerged as a transformative approach to enhance image generation and editing capabilities. Although integrating diffusion models with preference alignment strategies poses significant challenges for novices at this intersection, comprehensive and systematic reviews of this subject are still notably lacking. To bridge this gap, this paper extensively surveys preference alignment with diffusion models in image generation and editing. First, we systematically review cutting-edge optimization techniques such as reinforcement learning with human feedback (RLHF), direct preference optimization (DPO), and others, highlighting their pivotal role in aligning preferences with DMs. Then, we thoroughly explore the applications of aligning preferences with DMs in autonomous driving, medical imaging, robotics, and more. Finally, we comprehensively discuss the challenges of preference alignment with DMs. To our knowledge, this is the first survey centered on preference alignment with DMs, providing insights to drive future innovation in this dynamic area.",
    "zhlink": "https://martinspace.top/zh/dmrl-survey/",
    "enlink": "https://arxiv.org/abs/2502.07829"
  },
  {
    "id": 2,
    "title": "MuralXGAN: Text-Guided Dunhuang Mural Image Inpainting Framework",
    "conference": "",
    "cover_image": "arch.jpg",
    "year": "May, 2025",
    "abstract": "MuralXGAN, a novel text-guided image inpainting framework designed for the restoration of ancient Dunhuang murals. Traditional mural restoration approaches often rely on expert-drawn structural lines, which are labour-intensive and limited in availability. To address this, we propose a cross-modal system that uses large language models and a fine-tuned CLIP encoder to generate and process damage-related captions, enabling semantic and culturally informed restoration. Leveraging the MuralDH dataset, we implement a multi-stage architecture consisting of caption generation, cross-attention feature fusion, coarse inpainting, fine color correction, and adversarial refinement. To compensate for the lack of ground truth, we employ a random mask permutation strategy, improving model generalization. Quantitative experiments show that MuralXGAN outperforms a no-text baseline and state-of-the-art approaches in metrics such as PSNR, MAE, and SSIM.",
    "enlink": "https://github.com/Mural-inpaint/MuralXGAN"
  },
  {
    "id": 2,
    "title": "From RNN to Transformers",
    "conference": "Martin-Space Blog",
    "cover_image": "whatisrnn.jpg",
    "year": "Jan, 2025",
    "abstract": "How to make machines understand natural language? Take this Journey - Building a Knowledge Hierachy for Large Language Models from the Scratch!",
    "zhlink": "https://martinspace.top/zh/nluplus/"
  },
  {
    "id": 3,
    "title": "Serverless LoRA Serving",
    "conference": "Martin-Space Blog",
    "cover_image": "mlsys.jpg",
    "year": "May, 2025",
    "abstract": "Recently, we explore how Low-Rank Adaptation (LoRA) fine-tuning can be effectively combined with serverless inference using the ServerlessLLM framework. It addresses key challenges of deploying large language models (LLMs) in serverless environments, such as cold start latency, GPU resource demands, and the need for stateful processing. ServerlessLLM mitigates these issues through chunk-based model loading, live migration, and startup-aware scheduling. LoRA's lightweight nature makes it ideal for serverless use—enabling quick adapter loading, dynamic switching, and cost-efficient deployment. The implementation supports transformers backend, adapter hot-swapping, and efficient resource use. Future plans include supporting full fine-tuning, adapter fusion, and a dedicated backend for tuning, aiming to make LLM deployment and experimentation more accessible to resource-constrained users.",
    "zhlink": "https://martinspace.top/zh/serverless-lora/"
  },
  {
    "id": 4,
    "title": "Reinforcement Learning",
    "conference": "Martin-Space Blog",
    "year": "Jan, 2025",
    "abstract": "",
    "zhlink": "https://martinspace.top/zh/rl-note/"
  },
  {
    "id": 5,
    "title": "Conformal Prediction",
    "year": 2025,
    "abstract": "Something about uncertainty quantification",
    "zhlink": "https://scholar.google.com/"
  },
  {
    "id": 6,
    "title": "Llama-3-70B Quatisation Note",
    "year": 2024,
    "abstract": "LLM quantisation work note",
    "zhlink": "https://scholar.google.com/"
  },
  {
    "id": 7,
    "title": "N8CIR Bede Internship Note",
    "year": 2024,
    "abstract": "Blog of my internship",
    "zhlink": "https://scholar.google.com/"
  },
  {
    "id": 8,
    "title": "饮茶笔记",
    "year": 2023,
    "abstract": "A story about a Pu'er Tea beginner player",
    "zhlink": "https://scholar.google.com/"
  }
]